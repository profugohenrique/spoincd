# -*- coding: utf-8 -*-
"""INCDRegressaoLogistica.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hOM7pJS_FHrLruoIP_h_pMh2C0FRd8Fd
"""

# Importando bibliotecas necessárias
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, confusion_matrix,
                           classification_report, roc_curve, auc,
                           precision_recall_curve)
from sklearn.datasets import make_classification
import joblib
import warnings
warnings.filterwarnings('ignore')

# Configuração de estilo para os gráficos
plt.style.use('seaborn-v0_8')
np.random.seed(42)  # Para reproducibilidade

# 1. Geração de Dados Sintéticos
print("1. GERANDO DADOS SINTÉTICOS...")
print("-" * 50)

# Criando dados sintéticos para o exemplo
n_samples = 1000
X, y = make_classification(
    n_samples=n_samples,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=42
)

# Transformando para ter valores mais interpretáveis
X = X * 10 + 50
X[:, 0] = np.clip(X[:, 0], 20, 80)  # Horas de estudo: 20-80
X[:, 1] = np.clip(X[:, 1], 4, 10)   # Horas de sono: 4-10

# Criando DataFrame
df = pd.DataFrame(X, columns=['horas_estudo', 'horas_sono'])
df['aprovado'] = y

print(f"Shape dos dados: {df.shape}")
print(f"\nDistribuição das classes:\n{df['aprovado'].value_counts()}")

# 2. Análise Exploratória dos Dados
print("\n\n2. ANÁLISE EXPLORATÓRIA")
print("-" * 50)

# Correlação entre variáveis
correlation = df.corr()
print("Matriz de correlação:")
print(correlation)

# 3. Pré-processamento dos Dados
print("\n\n3. PRÉ-PROCESSAMENTO")
print("-" * 50)

X = df[['horas_estudo', 'horas_sono']]
y = df['aprovado']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Normalizando os dados
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convertendo de volta para DataFrame para manter nomes das features
X_train_scaled_df = pd.DataFrame(X_train_scaled,
                                columns=X_train.columns,
                                index=X_train.index)
X_test_scaled_df = pd.DataFrame(X_test_scaled,
                               columns=X_test.columns,
                               index=X_test.index)

# 4. Treinamento do Modelo
print("\n\n4. TREINAMENTO DO MODELO")
print("-" * 50)

model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train_scaled_df, y_train)

print("Modelo treinado com sucesso!")
print(f"Coeficientes: {model.coef_}")
print(f"Intercepto: {model.intercept_}")

# 5. Previsões e Avaliação
print("\n\n5. AVALIAÇÃO DO MODELO")
print("-" * 50)

y_pred = model.predict(X_test_scaled_df)
y_pred_proba = model.predict_proba(X_test_scaled_df)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Acurácia: {accuracy:.4f}")
print(f"\nMatriz de Confusão:\n{conf_matrix}")
print(f"\nRelatório de Classificação:\n{class_report}")

# 6. Visualização dos Resultados
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Matriz de Confusão
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
           xticklabels=['Não Aprovado', 'Aprovado'],
           yticklabels=['Não Aprovado', 'Aprovado'], ax=axes[0, 0])
axes[0, 0].set_title('Matriz de Confusão')

# Curva ROC
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
axes[0, 1].plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.3f})')
axes[0, 1].plot([0, 1], [0, 1], 'k--')
axes[0, 1].set_title('Curva ROC')
axes[0, 1].legend()

# Curva Precision-Recall
precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
axes[1, 0].plot(recall, precision, label='Precision-Recall')
axes[1, 0].set_title('Curva Precision-Recall')
axes[1, 0].legend()

# Distribuição das Probabilidades
sns.histplot(y_pred_proba, bins=30, kde=True, ax=axes[1, 1])
axes[1, 1].axvline(0.5, color='red', linestyle='--')
axes[1, 1].set_title('Distribuição das Probabilidades')

plt.tight_layout()
plt.show()

# 7. Interpretação do Modelo
print("\n\n7. INTERPRETAÇÃO DO MODELO")
print("-" * 50)

coef_original = model.coef_ / scaler.scale_
intercept_original = model.intercept_ - np.sum(model.coef_ * scaler.mean_ / scaler.scale_)

print("Equação da regressão logística:")
print(f"log(p/(1-p)) = {intercept_original[0]:.4f} + {coef_original[0][0]:.4f}*horas_estudo + {coef_original[0][1]:.4f}*horas_sono")

# Odds Ratio
odds_ratios = np.exp(model.coef_[0])
print(f"\nOdds Ratios:")
for feature, odds_ratio in zip(X.columns, odds_ratios):
    print(f"- {feature}: {odds_ratio:.4f}")

# 8. Exemplo de Previsão para Novos Dados
print("\n\n8. EXEMPLO DE PREVISÃO")
print("-" * 50)

# Criando DataFrame para manter os nomes das features
novos_dados = pd.DataFrame({
    'horas_estudo': [60, 30, 70],
    'horas_sono': [7, 5, 8]
})

# Pré-processamento correto
novos_dados_scaled = scaler.transform(novos_dados)
novos_dados_scaled_df = pd.DataFrame(novos_dados_scaled,
                                   columns=novos_dados.columns)

previsoes = model.predict(novos_dados_scaled_df)
probabilidades = model.predict_proba(novos_dados_scaled_df)

print("Previsões para novos dados:")
for i, (_, row) in enumerate(novos_dados.iterrows()):
    status = "APROVADO" if previsoes[i] == 1 else "REPROVADO"
    print(f"Exemplo {i+1}: {row['horas_estudo']}h estudo, {row['horas_sono']}h sono")
    print(f"  Status: {status}")
    print(f"  Probabilidade de aprovação: {probabilidades[i][1]:.2%}")
    print()

# 9. Salvando o Modelo
print("\n\n9. SALVANDO O MODELO")
print("-" * 50)

# Salvando modelo e scaler
joblib.dump(model, 'modelo_regressao_logistica.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(X.columns.tolist(), 'feature_names.pkl')

print(" Modelo salvo com sucesso!")
print(" Arquivos criados:")
print("   - modelo_regressao_logistica.pkl")
print("   - scaler.pkl")
print("   - feature_names.pkl")

# 10. Carregando e Usando o Modelo Salvo
print("\n\n10. CARREGANDO E TESTANDO O MODELO SALVO")
print("-" * 50)

# Carregando o modelo salvo
model_loaded = joblib.load('modelo_regressao_logistica.pkl')
scaler_loaded = joblib.load('scaler.pkl')
feature_names = joblib.load('feature_names.pkl')

# Testando com os mesmos dados
novos_dados_test = pd.DataFrame({
    'horas_estudo': [55, 25],
    'horas_sono': [6, 4]
}, columns=feature_names)

# Pré-processamento
novos_dados_scaled_test = scaler_loaded.transform(novos_dados_test)
novos_dados_scaled_df_test = pd.DataFrame(novos_dados_scaled_test,
                                        columns=feature_names)

# Previsão
previsoes_test = model_loaded.predict(novos_dados_scaled_df_test)
probabilidades_test = model_loaded.predict_proba(novos_dados_scaled_df_test)

print("Teste do modelo carregado:")
for i, (_, row) in enumerate(novos_dados_test.iterrows()):
    status = "APROVADO" if previsoes_test[i] == 1 else "REPROVADO"
    print(f"Exemplo {i+1}: {row['horas_estudo']}h estudo, {row['horas_sono']}h sono")
    print(f"  Status: {status}")
    print(f"  Probabilidade: {probabilidades_test[i][1]:.2%}")
    print()

print("Modelo carregado e testado com sucesso!")